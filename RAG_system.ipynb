{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf7e05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: arxiv in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
      "\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain-community in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (0.3.45)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (0.3.21)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (3.11.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (0.3.11)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: wikipedia in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from wikipedia) (4.13.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (4.12.2)\n",
      "\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain-openai in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (0.3.9)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-openai) (0.3.45)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.66.3 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-openai) (1.66.5)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (0.3.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (2.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.66.3->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain-openai) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
      "\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain-google-genai in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (2.0.11)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-google-genai) (0.6.16)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.37 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-google-genai) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3,>=2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-google-genai) (2.10.6)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.3)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.3.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.1)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0rc2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0rc2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.23.0)\n",
      "Requirement already satisfied: anyio in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.14.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.3.1)\n",
      "\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: faiss-cpu in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from faiss-cpu) (24.2)\n",
      "\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: entrezpy in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (2.1.3)\n",
      "\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pymupdf in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (1.25.3)\n",
      "\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: beautifulsoup4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (4.13.3)\n",
      "Requirement already satisfied: requests in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: crawl4ai in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (0.5.0.post4)\n",
      "Requirement already satisfied: aiosqlite~=0.20 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (0.21.0)\n",
      "Requirement already satisfied: lxml~=5.3 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (5.3.1)\n",
      "Requirement already satisfied: litellm>=1.53.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (1.63.6)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (1.26.4)\n",
      "Requirement already satisfied: pillow~=10.4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (10.4.0)\n",
      "Requirement already satisfied: playwright>=1.49.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (1.50.0)\n",
      "Requirement already satisfied: python-dotenv~=1.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (1.0.1)\n",
      "Requirement already satisfied: requests~=2.26 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4~=4.12 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (4.13.3)\n",
      "Requirement already satisfied: tf-playwright-stealth>=1.1.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (1.1.2)\n",
      "Requirement already satisfied: xxhash~=3.4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (3.5.0)\n",
      "Requirement already satisfied: rank-bm25~=0.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (0.2.2)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (24.1.0)\n",
      "Requirement already satisfied: colorama~=0.4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (0.4.6)\n",
      "Requirement already satisfied: snowballstemmer~=2.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (2.2.0)\n",
      "Requirement already satisfied: pydantic>=2.10 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (2.10.6)\n",
      "Requirement already satisfied: pyOpenSSL>=24.3.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (25.0.0)\n",
      "Requirement already satisfied: psutil>=6.1.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (7.0.0)\n",
      "Requirement already satisfied: nltk>=3.9.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (3.9.1)\n",
      "Requirement already satisfied: rich>=13.9.4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (13.9.4)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.27.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (0.28.1)\n",
      "Requirement already satisfied: fake-useragent>=2.0.3 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (2.0.3)\n",
      "Requirement already satisfied: click>=8.1.7 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (8.1.8)\n",
      "Requirement already satisfied: pyperclip>=1.8.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (1.9.0)\n",
      "Requirement already satisfied: faust-cchardet>=2.1.19 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (2.1.19)\n",
      "Requirement already satisfied: aiohttp>=3.11.11 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (3.11.13)\n",
      "Requirement already satisfied: humanize>=4.10.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from crawl4ai) (4.12.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (1.18.3)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from aiosqlite~=0.20->crawl4ai) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from beautifulsoup4~=4.12->crawl4ai) (2.6)\n",
      "Requirement already satisfied: anyio in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx>=0.27.2->crawl4ai) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx>=0.27.2->crawl4ai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx>=0.27.2->crawl4ai) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpx>=0.27.2->crawl4ai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.2->crawl4ai) (0.14.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (8.6.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (3.1.5)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.61.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (1.66.5)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (0.9.0)\n",
      "Requirement already satisfied: tokenizers in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (0.21.0)\n",
      "Requirement already satisfied: joblib in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from nltk>=3.9.1->crawl4ai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from nltk>=3.9.1->crawl4ai) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
      "Requirement already satisfied: pyee<13,>=12 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from playwright>=1.49.0->crawl4ai) (12.1.1)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from playwright>=1.49.0->crawl4ai) (3.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from pydantic>=2.10->crawl4ai) (2.27.2)\n",
      "Requirement already satisfied: cryptography<45,>=41.0.5 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from pyOpenSSL>=24.3.0->crawl4ai) (44.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests~=2.26->crawl4ai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from requests~=2.26->crawl4ai) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from rich>=13.9.4->crawl4ai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from rich>=13.9.4->crawl4ai) (2.15.1)\n",
      "Requirement already satisfied: fake-http-header<0.4.0,>=0.3.5 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from tf-playwright-stealth>=1.1.0->crawl4ai) (0.3.5)\n",
      "Requirement already satisfied: cffi>=1.12 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (1.17.1)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.23.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai) (0.1.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from openai>=1.61.0->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from openai>=1.61.0->litellm>=1.53.1->crawl4ai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from openai>=1.61.0->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg (from tokenizers->litellm>=1.53.1->crawl4ai) (0.29.1)\n",
      "Requirement already satisfied: pycparser in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from cffi>=1.12->cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.22)\n",
      "Requirement already satisfied: filelock in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\n",
      "\u001b[33mDEPRECATION: Loading egg at /home/adityasj/.conda/envs/Adityaenv/lib/python3.11/site-packages/huggingface_hub-0.29.1-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: python-dotenv in ./.conda/envs/Adityaenv/lib/python3.11/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv\n",
    "!pip install -U langchain-community\n",
    "!pip install wikipedia\n",
    "!pip install -U langchain-openai\n",
    "!pip install langchain-google-genai\n",
    "!pip install faiss-cpu\n",
    "!pip install google-api-python-client>=2.100.0\n",
    "!pip install entrezpy\n",
    "!pip install pymupdf\n",
    "!pip install beautifulsoup4 requests\n",
    "!pip install crawl4ai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe0dc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import Document\n",
    "from langchain.tools import Tool\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "from typing import List\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
    "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
    "import requests\n",
    "from xml.etree import ElementTree\n",
    "import webbrowser\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a265fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "google_cse_id = os.getenv(\"GOOGLE_CSE_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc860cfc",
   "metadata": {},
   "source": [
    "# Wikipedia API for Document retrieval.!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88518cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_wrapper=WikipediaAPIWrapper(top_k_results=3,doc_content_chars_max=200)\n",
    "# wiki=WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "class WikipediaRetriever:\n",
    "    def __init__(self, top_k_results=3, doc_content_chars_max=200):\n",
    "        self.wrapper = WikipediaAPIWrapper(top_k_results=top_k_results, doc_content_chars_max=doc_content_chars_max)\n",
    "\n",
    "    def search(self, query):\n",
    "        \"\"\"Fetches Wikipedia content and returns both text and URLs.\"\"\"\n",
    "        result = self.wrapper.run(query)  # Retrieve Wikipedia content\n",
    "        urls = [f\"https://en.wikipedia.org/wiki/{query.replace(' ', '_')}\"]  # Construct Wikipedia URL\n",
    "        return {\"content\": result, \"urls\": urls}\n",
    "\n",
    "# Initialize Wikipedia Tool\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=3, doc_content_chars_max=200)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "def wikipedia_with_urls(query):\n",
    "    retriever = WikipediaRetriever()\n",
    "    result = retriever.search(query)  # Get content + URL\n",
    "\n",
    "    # ✅ Return formatted string so the LLM does not need to process `{url}`\n",
    "    return f\"**Response:** {result['content']}\\n\\n**Source:** [Click here]({result['urls'][0]})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "566a2b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dd6367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key = google_api_key,\n",
    "    google_csi_id = google_csi_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9ddb58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1555176ef650>, search_kwargs={})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load and split documents\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)\n",
    "\n",
    "# Create FAISS VectorStore with Google Embeddings\n",
    "vectordb = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35f0041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(retriever,\"langsmith_search\",\n",
    "                      \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c45d849e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith_search'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370e6ae",
   "metadata": {},
   "source": [
    "# Arxiv tool for document search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c894dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arxiv Tool\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=3, doc_content_chars_max=200)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "\n",
    "def arxiv_with_urls(query):\n",
    "    \"\"\"Fetches ArXiv papers and returns both content & URLs.\"\"\"\n",
    "    results = arxiv.run(query)  # Retrieve papers\n",
    "\n",
    "    extracted_results = []\n",
    "    for paper in results:\n",
    "        extracted_results.append({\n",
    "            \"content\": f\"Title: {paper.title}\\nSummary: {paper.summary}\",\n",
    "            \"url\": paper.entry_id  # ✅ ArXiv provides direct links in `entry_id`\n",
    "        })\n",
    "\n",
    "    return extracted_results if extracted_results else [{\"content\": \"No papers found.\", \"url\": \"No URL available.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb564ef",
   "metadata": {},
   "source": [
    "# Google scholar API for document search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aac48d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2037998/3922096516.py:8: LangChainDeprecationWarning: The class `GoogleSearchAPIWrapper` was deprecated in LangChain 0.0.33 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-google-community package and should be used instead. To use it run `pip install -U :class:`~langchain-google-community` and import as `from :class:`~langchain_google_community import GoogleSearchAPIWrapper``.\n",
      "  google_scholar_wrapper = GoogleSearchAPIWrapper(\n",
      "/tmp/ipykernel_2037998/3922096516.py:12: LangChainDeprecationWarning: The class `GoogleSearchRun` was deprecated in LangChain 0.0.33 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-google-community package and should be used instead. To use it run `pip install -U :class:`~langchain-google-community` and import as `from :class:`~langchain_google_community import GoogleSearchRun``.\n",
      "  google_scholar = GoogleSearchRun(api_wrapper=google_scholar_wrapper)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_community.tools import GoogleSearchRun\n",
    "from langchain.tools import Tool\n",
    "\n",
    "# ✅ Initialize Google Search API\n",
    "google_scholar_wrapper = GoogleSearchAPIWrapper(\n",
    "    google_api_key=google_api_key,  \n",
    "    google_cse_id=google_cse_id\n",
    ")\n",
    "google_scholar = GoogleSearchRun(api_wrapper=google_scholar_wrapper)\n",
    "\n",
    "# ✅ Function to Fetch Google Scholar Results with URLs\n",
    "def google_scholar_with_urls(query):\n",
    "    \"\"\"Fetches Google Scholar results and returns both content & URLs.\"\"\"\n",
    "    results = google_scholar.run(query)  # Retrieve search results\n",
    "\n",
    "    extracted_results = []\n",
    "    for entry in results:\n",
    "        extracted_results.append({\n",
    "            \"content\": f\"Title: {entry['title']}\\nSnippet: {entry['snippet']}\",\n",
    "            \"url\": entry['link']  # ✅ Extracts the link to the source\n",
    "        })\n",
    "\n",
    "    return extracted_results if extracted_results else [{\"content\": \"No results found.\", \"url\": \"No URL available.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17ceb1",
   "metadata": {},
   "source": [
    "# Source: Pubmed dataset\n",
    "\n",
    "## Implementing FAISS for embedding and faster retrieval, since Pubmed documents can be of larger size.!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77f2b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class PubMedLoader:\n",
    "    def __init__(self, query, top_k=3):\n",
    "        self.query = query\n",
    "        self.top_k = top_k\n",
    "        self.api_key = os.getenv(\"PUBMED_API_KEY\")\n",
    "        self.base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "\n",
    "    def fetch_article_ids(self):\n",
    "        \"\"\"Fetches PubMed article IDs for a given query.\"\"\"\n",
    "        params = {\n",
    "            \"db\": \"pubmed\",\n",
    "            \"term\": self.query,\n",
    "            \"retmode\": \"json\",\n",
    "            \"retmax\": self.top_k,\n",
    "            \"api_key\": self.api_key\n",
    "        }\n",
    "        response = requests.get(self.base_url, params=params).json()\n",
    "        return response.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "\n",
    "    def fetch_article_abstracts(self, article_ids):\n",
    "        \"\"\"Fetches full abstracts for given PubMed article IDs.\"\"\"\n",
    "        articles = []\n",
    "        for article_id in article_ids:\n",
    "            url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={article_id}&retmode=text&rettype=abstract&api_key={self.api_key}\"\n",
    "            article_text = requests.get(url).text\n",
    "            articles.append(f\"PubMed ID: {article_id}\\n\\n{article_text}\")\n",
    "        return articles\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Fetches articles and returns them as raw text.\"\"\"\n",
    "        article_ids = self.fetch_article_ids()\n",
    "        if not article_ids:\n",
    "            return []\n",
    "\n",
    "        return self.fetch_article_abstracts(article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "104c76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Google AI Embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Initialize FAISS as None (will be created when data is available)\n",
    "vectordb = None\n",
    "retriever = None\n",
    "\n",
    "# Text Splitter for chunking abstracts before storing\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "def retrieve_pubmed_articles(query):\n",
    "    global vectordb, retriever  \n",
    "\n",
    "    # Check FAISS first\n",
    "    if retriever:\n",
    "        similar_docs = retriever.get_relevant_documents(query)\n",
    "        if similar_docs:\n",
    "            print(\" Retrieving from FAISS (Cached Results)\")\n",
    "            return similar_docs\n",
    "\n",
    "    print(\" No Cached Results, Calling PubMed API...\")\n",
    "    \n",
    "    # Fetch fresh articles from PubMed API\n",
    "    pubmed_loader = PubMedLoader(query, top_k=3)\n",
    "    docs = pubmed_loader.load()\n",
    "\n",
    "    if not docs:\n",
    "        print(\" No articles retrieved from PubMed.\")\n",
    "        return []\n",
    "\n",
    "    print(f\" Retrieved {len(docs)} articles from PubMed\")  # Debugging line\n",
    "\n",
    "    # Store new results in FAISS only if there are documents\n",
    "    doc_objects = [Document(page_content=text) for text in docs]\n",
    "\n",
    "    if doc_objects:\n",
    "        chunked_documents = text_splitter.split_documents(doc_objects)\n",
    "\n",
    "        # Debugging before storing in FAISS\n",
    "        print(f\" Storing {len(chunked_documents)} documents in FAISS\")\n",
    "\n",
    "        vectordb = FAISS.from_documents(chunked_documents, embeddings)\n",
    "        retriever = vectordb.as_retriever()\n",
    "\n",
    "        print(\" New results stored in FAISS for future queries.\")\n",
    "\n",
    "    return doc_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9ad625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Artificial Intelligence in Healthcare\"\n",
    "# results = retrieve_pubmed_articles(query)\n",
    "\n",
    "# # Print results\n",
    "# for idx, result in enumerate(results):\n",
    "#     print(f\"\\n🔹 Result {idx+1}:\\n{result.page_content[:1000]}...\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17a7d8f",
   "metadata": {},
   "source": [
    "# Educational pdfs and textbooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffe1ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from crawl4ai import Crawler\n",
    "\n",
    "class OpenTextbookCrawler:\n",
    "    def __init__(self, base_url=\"https://open.umn.edu/opentextbooks\", top_k=3):\n",
    "        self.base_url = base_url\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def fetch_textbooks(self):\n",
    "        \"\"\"Uses Crawl4AI to scrape Open Textbook Library for book titles & PDF links.\"\"\"\n",
    "        crawler = Crawler()\n",
    "        response = crawler.get(self.base_url)\n",
    "\n",
    "        books = []\n",
    "        for book in response.extract_all({\"title\": \"//h4/a\", \"pdf_link\": \"//a[contains(@href, 'download')]\"}):\n",
    "            title = book.get(\"title\", \"Unknown Title\").strip()\n",
    "            pdf_link = book.get(\"pdf_link\", None)\n",
    "\n",
    "            if pdf_link:\n",
    "                books.append({\"title\": title, \"pdf_link\": pdf_link})\n",
    "\n",
    "        return books[:self.top_k]  # Return top K books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7220b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import requests\n",
    "import os\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, pdf_url, save_path=\"downloads\"):\n",
    "        self.pdf_url = pdf_url\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def download_pdf(self):\n",
    "        \"\"\"Downloads the PDF from the provided URL.\"\"\"\n",
    "        response = requests.get(self.pdf_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            pdf_path = os.path.join(self.save_path, self.pdf_url.split(\"/\")[-1])\n",
    "\n",
    "            with open(pdf_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            return pdf_path\n",
    "        return None\n",
    "\n",
    "    def extract_text(self, pdf_path):\n",
    "        \"\"\"Extracts text from the given PDF.\"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\\n\".join(page.get_text() for page in doc)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19359ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize FAISS components\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Initialize FAISS as None (will be created dynamically)\n",
    "vectordb = None\n",
    "retriever = None\n",
    "\n",
    "# Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "def process_textbook(pdf_url):\n",
    "    global vectordb, retriever  \n",
    "\n",
    "    # Download and extract text\n",
    "    processor = PDFProcessor(pdf_url)\n",
    "    pdf_path = processor.download_pdf()\n",
    "\n",
    "    if not pdf_path:\n",
    "        print(\" Failed to download PDF.\")\n",
    "        return []\n",
    "\n",
    "    extracted_text = processor.extract_text(pdf_path)\n",
    "    \n",
    "    # Convert to LangChain Documents\n",
    "    doc_objects = [Document(page_content=extracted_text)]\n",
    "\n",
    "    if doc_objects:\n",
    "        chunked_documents = text_splitter.split_documents(doc_objects)\n",
    "\n",
    "        # Initialize FAISS only when we have data\n",
    "        vectordb = FAISS.from_documents(chunked_documents, embeddings)\n",
    "        retriever = vectordb.as_retriever()\n",
    "\n",
    "        print(\" Textbook stored in FAISS for future retrieval.\")\n",
    "\n",
    "    return doc_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4aff6969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize FAISS components\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Initialize FAISS as None (will be created dynamically)\n",
    "vectordb = None\n",
    "retriever = None\n",
    "\n",
    "# Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "def process_textbook(pdf_url):\n",
    "    global vectordb, retriever  \n",
    "\n",
    "    # Download and extract text\n",
    "    processor = PDFProcessor(pdf_url)\n",
    "    pdf_path = processor.download_pdf()\n",
    "\n",
    "    if not pdf_path:\n",
    "        print(\" Failed to download PDF.\")\n",
    "        return []\n",
    "\n",
    "    extracted_text = processor.extract_text(pdf_path)\n",
    "    \n",
    "    # Convert to LangChain Documents\n",
    "    doc_objects = [Document(page_content=extracted_text)]\n",
    "\n",
    "    if doc_objects:\n",
    "        chunked_documents = text_splitter.split_documents(doc_objects)\n",
    "\n",
    "        # Initialize FAISS only when we have data\n",
    "        vectordb = FAISS.from_documents(chunked_documents, embeddings)\n",
    "        retriever = vectordb.as_retriever()\n",
    "\n",
    "        print(\" Textbook stored in FAISS for future retrieval.\")\n",
    "\n",
    "    return doc_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8263299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize FAISS components\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Initialize FAISS as None (will be created dynamically)\n",
    "vectordb = None\n",
    "retriever = None\n",
    "\n",
    "# Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "def process_textbook(pdf_url):\n",
    "    global vectordb, retriever  \n",
    "\n",
    "    # Download and extract text\n",
    "    processor = PDFProcessor(pdf_url)\n",
    "    pdf_path = processor.download_pdf()\n",
    "\n",
    "    if not pdf_path:\n",
    "        print(\" Failed to download PDF.\")\n",
    "        return []\n",
    "\n",
    "    extracted_text = processor.extract_text(pdf_path)\n",
    "    \n",
    "    # Convert to LangChain Documents\n",
    "    doc_objects = [Document(page_content=extracted_text)]\n",
    "\n",
    "    if doc_objects:\n",
    "        chunked_documents = text_splitter.split_documents(doc_objects)\n",
    "\n",
    "        # Initialize FAISS only when we have data\n",
    "        vectordb = FAISS.from_documents(chunked_documents, embeddings)\n",
    "        retriever = vectordb.as_retriever()\n",
    "\n",
    "        print(\" Textbook stored in FAISS for future retrieval.\")\n",
    "\n",
    "    return doc_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "888e81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_textbook(query):\n",
    "    global vectordb, retriever  \n",
    "\n",
    "    # 1️⃣ Check FAISS First\n",
    "    if retriever:\n",
    "        similar_docs = retriever.get_relevant_documents(query)\n",
    "        if similar_docs:\n",
    "            print(\" Retrieving from FAISS (Cached Results)\")\n",
    "            return similar_docs\n",
    "\n",
    "    print(\" No Cached Results, Scraping New Textbooks...\")\n",
    "\n",
    "    # 2️⃣ Scrape books from Open Textbook Library\n",
    "    scraper = OpenTextbookCrawler(top_k=3)\n",
    "    books = scraper.fetch_textbooks()\n",
    "\n",
    "    if not books:\n",
    "        print(\" No textbooks found.\")\n",
    "        return []\n",
    "\n",
    "    # 3️⃣ Process the first book (For now, only handling 1 book at a time)\n",
    "    book = books[0]\n",
    "    print(f\" Downloading textbook: {book['title']}\")\n",
    "    \n",
    "    return process_textbook(book[\"pdf_link\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a35fd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65445992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6afce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5e9ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia Tool\n",
    "wikipedia_tool = Tool(\n",
    "    name=\"Wikipedia_Search\",  # ✅ Name must be valid for Gemini API\n",
    "    func=wikipedia_with_urls,  # Calls the function we modified\n",
    "    description=\"Search for Wikipedia articles on a given topic. Returns both content and source URL.\"\n",
    ")\n",
    "\n",
    "# ArXiv Tool\n",
    "arxiv_tool = Tool(\n",
    "    name=\"ArXiv_Research\",\n",
    "    func=arxiv_with_urls,  # ✅ Uses the new function\n",
    "    description=\"Retrieve academic research papers from ArXiv. Returns both content and source URL.\"\n",
    ")\n",
    "\n",
    "# Google Scholar Tool\n",
    "google_scholar_tool = Tool(\n",
    "    name=\"Google_Scholar_Search\",\n",
    "    func=google_scholar_with_urls,  # ✅ Uses the updated function\n",
    "    description=\"Search for academic articles and research papers from Google Scholar. Returns both content and source URL.\"\n",
    ")\n",
    "\n",
    "# PubMed Tool\n",
    "pubmed_tool = Tool(\n",
    "    name=\"PubMed_Search\",\n",
    "    func=retrieve_pubmed_articles,  # Using the PubMed retrieval function\n",
    "    description=\"Search for academic research papers from PubMed based on a given query. Use this tool for medical and scientific topics.\"\n",
    ")\n",
    "\n",
    "open_textbook_tool = Tool(\n",
    "    name=\"Open_Textbook_Library\",\n",
    "    func=retrieve_textbook,\n",
    "    description=\"Retrieve educational textbooks from Open Textbook Library. Use this tool for academic references.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9007251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wikipedia_tool,arxiv_tool,google_scholar_tool, retriever_tool, pubmed_tool, open_textbook_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e17d2920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='Wikipedia_Search', description='Search for Wikipedia articles on a given topic. Returns both content and source URL.', func=<function wikipedia_with_urls at 0x1555175ae5c0>),\n",
       " Tool(name='ArXiv_Research', description='Retrieve academic research papers from ArXiv. Returns both content and source URL.', func=<function arxiv_with_urls at 0x15552aeed800>),\n",
       " Tool(name='Google_Scholar_Search', description='Search for academic articles and research papers from Google Scholar. Returns both content and source URL.', func=<function google_scholar_with_urls at 0x1554bd55d3a0>),\n",
       " Tool(name='langsmith_search', description='Search for information about LangSmith. For any questions about LangSmith, you must use this tool!', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x15552b402b60>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1555176ef650>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x15552aeed300>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1555176ef650>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content')),\n",
       " Tool(name='PubMed_Search', description='Search for academic research papers from PubMed based on a given query. Use this tool for medical and scientific topics.', func=<function retrieve_pubmed_articles at 0x1554bd55ec00>),\n",
       " Tool(name='Open_Textbook_Library', description='Retrieve educational textbooks from Open Textbook Library. Use this tool for academic references.', func=<function retrieve_textbook at 0x1554bd55d8a0>)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "322d956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.tools import Tool\n",
    "\n",
    "# for tool in tools:\n",
    "#     if isinstance(tool, Tool):  # ✅ Only print if it's a Tool\n",
    "#         print(f\"🔹 Tool Name: {tool.name}, Function: {tool.func}\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ Non-tool object found: {tool}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63b4ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Machine Learning\"\n",
    "# textbook_results = open_textbook_tool.func(query)\n",
    "\n",
    "# # Print results\n",
    "# for idx, result in enumerate(textbook_results):\n",
    "#     print(f\"\\n🔹 Textbook Result {idx+1}:\\n{result.page_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71bd967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fe6fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-pro-exp-02-05\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "340f754b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. When answering a question, always include both the content and the source URL if available.'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain import hub\n",
    "# # Get the prompt to use - you can modify this!\n",
    "# prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "# prompt.messages.insert(0, \"When answering a question, always include both the content and the source URL if available.\")\n",
    "# prompt.messages\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "updated_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are a helpful assistant. When answering a question, always include both the content and the source URL if available.\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "updated_prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a2be1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agents\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,updated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c29588d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x15552b570a40>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x15552b570a40>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. When answering a question, always include both the content and the source URL if available.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-2.0-pro-exp-02-05', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x1554bb485fd0>, default_metadata=()), kwargs={'tools': [{'type': 'function', 'function': {'name': 'Wikipedia_Search', 'description': 'Search for Wikipedia articles on a given topic. Returns both content and source URL.', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'ArXiv_Research', 'description': 'Retrieve academic research papers from ArXiv. Returns both content and source URL.', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'Google_Scholar_Search', 'description': 'Search for academic articles and research papers from Google Scholar. Returns both content and source URL.', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith_search', 'description': 'Search for information about LangSmith. For any questions about LangSmith, you must use this tool!', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'PubMed_Search', 'description': 'Search for academic research papers from PubMed based on a given query. Use this tool for medical and scientific topics.', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'Open_Textbook_Library', 'description': 'Retrieve educational textbooks from Open Textbook Library. Use this tool for academic references.', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[Tool(name='Wikipedia_Search', description='Search for Wikipedia articles on a given topic. Returns both content and source URL.', func=<function wikipedia_with_urls at 0x1555175ae5c0>), Tool(name='ArXiv_Research', description='Retrieve academic research papers from ArXiv. Returns both content and source URL.', func=<function arxiv_with_urls at 0x15552aeed800>), Tool(name='Google_Scholar_Search', description='Search for academic articles and research papers from Google Scholar. Returns both content and source URL.', func=<function google_scholar_with_urls at 0x1554bd55d3a0>), Tool(name='langsmith_search', description='Search for information about LangSmith. For any questions about LangSmith, you must use this tool!', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x15552b402b60>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1555176ef650>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x15552aeed300>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1555176ef650>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content')), Tool(name='PubMed_Search', description='Search for academic research papers from PubMed based on a given query. Use this tool for medical and scientific topics.', func=<function retrieve_pubmed_articles at 0x1554bd55ec00>), Tool(name='Open_Textbook_Library', description='Retrieve educational textbooks from Open Textbook Library. Use this tool for academic references.', func=<function retrieve_textbook at 0x1554bd55d8a0>)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agent Executer\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf82ffe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "> Entering new AgentExecutor chain...\n",
      "\n",
      "Invoking: `Wikipedia_Search` with `{'query': 'Quantum Computing'}`\n",
      "\n",
      "\n",
      "**Response:** Page: Quantum computing\n",
      "Summary: A quantum computer is a computer that exploits quantum mechanical phenomena. On small scales, physical matter exhibits properties of both particles and waves, and quan\n",
      "\n",
      "**Source:** [Click here](https://en.wikipedia.org/wiki/Quantum_Computing)Quantum computing is a type of computation that harnesses the principles of quantum mechanics. Unlike classical computers that store information as bits representing 0 or 1, quantum computers use quantum bits, or qubits. Qubits can represent 0, 1, or a combination of both states simultaneously through a concept called superposition. This, along with other quantum phenomena like entanglement and interference, allows quantum computers to perform certain calculations much faster than classical computers.\n",
      "\n",
      "**Source:** [https://en.wikipedia.org/wiki/Quantum_Computing](https://en.wikipedia.org/wiki/Quantum_Computing)\n",
      "\n",
      "> Finished chain.\n",
      "\n",
      " Content:\n",
      "Quantum computing is a type of computation that harnesses the principles of quantum mechanics. Unlike classical computers that store information as bits representing 0 or 1, quantum computers use quantum bits, or qubits. Qubits can represent 0, 1, or a combination of both states simultaneously through a concept called superposition. This, along with other quantum phenomena like entanglement and interference, allows quantum computers to perform certain calculations much faster than classical computers.\n",
      "\n",
      "**Source:** [https://en.wikipedia.org/wiki/Quantum_Computing](https://en.wikipedia.org/wiki/Quantum_Computing)\n",
      " Source: No URL available.\n"
     ]
    }
   ],
   "source": [
    "query = \"Quantum Computing\"\n",
    "agent_response = agent_executor.invoke({\"input\": query})\n",
    "\n",
    "# Extract content and URL from agent response\n",
    "content = agent_response.get(\"output\", \"No content available.\")\n",
    "url = agent_response.get(\"url\", \"No URL available.\")  # Ensure it handles missing URLs gracefully\n",
    "\n",
    "# Print formatted response\n",
    "print(f\"\\n Content:\\n{content}\")\n",
    "print(f\" Source: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7053c7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41c4796e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m wiki_results \u001b[38;5;241m=\u001b[39m wikipedia_tool\u001b[38;5;241m.\u001b[39mfunc(query)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ✅ Extract content and first URL\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m content \u001b[38;5;241m=\u001b[39m wiki_results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo content available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m url \u001b[38;5;241m=\u001b[39m wiki_results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murls\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo URL available.\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# ✅ Ensure we get the first URL\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ✅ Print formatted response\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "query = \"Quantum Computing\"\n",
    "wiki_results = wikipedia_tool.func(query)\n",
    "\n",
    "# ✅ Extract content and first URL\n",
    "content = wiki_results.get(\"content\", \"No content available.\")\n",
    "url = wiki_results.get(\"urls\", [\"No URL available.\"])[0]  # ✅ Ensure we get the first URL\n",
    "\n",
    "# ✅ Print formatted response\n",
    "print(f\"\\n Wikipedia Content:\\n{content[:500]}...\")  # ✅ Truncate content for readability\n",
    "print(f\"🔗 Wikipedia Source: {url}\")  # ✅ Ensure proper URL extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a303b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Adityaenv",
   "language": "python",
   "name": "adityaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
